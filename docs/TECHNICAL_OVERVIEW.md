# Technical Overview: Model Ingestion, RAG, Chatbot, and API

This document describes how the ED Revisit Risk system works end-to-end: how models and knowledge bases are built and loaded, how the RAG-backed chatbot reasons over patient state, and how the API and frontend present the experience.

---

## Model and artifact ingestion

**Offline pipeline** (`scripts/run_end_to_end.sh`) drives ingestion. It reads `config.yaml` for paths, then: (1) loads the NHAMCS ED dataset (e.g. from a SAS ZIP), (2) builds **stats** (regional and condition-level 72-hour revisit and admission rates) and writes `artifacts/stats.json`, (3) builds the **RAG knowledge-base index** from markdown under `rag.kb_dir` (e.g. `med_proj/rag/knowledge_base/*.md`). The RAG index is produced in two ways: a **TF-IDF** index (scikit-learn `TfidfVectorizer`, document-level, saved as `artifacts/kb_index.joblib`) and, when optional dependencies are available, a **RapidFire-style embedding pipeline** that loads the same markdown with a LangChain `DirectoryLoader`, splits it with `RecursiveCharacterTextSplitter` (chunk size 256, overlap 32), embeds with HuggingFace `sentence-transformers/all-MiniLM-L6-v2`, and builds a **FAISS** vector index persisted under `artifacts/rag_faiss/`. A separately trained **XGBoost readmission model** (e.g. `artifacts/readmission_model.json`) is produced by the modeling pipeline and is **not** rebuilt by the end-to-end script; it must exist in the artifacts directory for the service to run in full. At **runtime**, the FastAPI app on startup instantiates a `ChatEngine` once to verify that the XGBoost model loads; the health check reports `models_loaded` and `rag_index_loaded` so the frontend can show “Model loaded” and whether the knowledge base is available.

---

## RAG (retrieval-augmented generation) and knowledge base

The **knowledge base** is a set of markdown files (e.g. clinical guidance, discharge best practices, condition-specific revisit literature). **Ingestion** produces either or both of: (1) a TF-IDF index (one vector per document, joblib) and (2) a FAISS index (chunk-level, dense embeddings). The **retrieval** layer (`med_proj/rag/retrieve.py`) prefers the FAISS index when `artifacts/rag_faiss/` exists: it loads the FAISS index and the same HuggingFace embedding model, runs a similarity search (L2 distance), and returns the top-k chunks with scores normalized to a 0–1 similarity. If FAISS is absent, it falls back to the TF-IDF index: the query is vectorized with the same `TfidfVectorizer`, cosine similarity is computed against the document matrix, and top-k documents are returned. In both cases the API returns a list of `{ score, source, excerpt }`. The chatbot uses this in two places: (1) **Ask intent** — when the user asks a clinical question (e.g. “What are risk factors for ED revisits?”), the message is used as the query and the top excerpts are formatted as “Knowledge Base Results” in the reply; (2) **Recommendations** — after an assessment, a synthetic query is built from risk level and the patient’s conditions (e.g. “high risk readmission recommendations”, “CHF”, “elderly”), and the top RAG hits are appended as a “Recommendations” section. So the RAG path does **not** generate free text; it **retrieves** stored passages and presents them as evidence alongside the model’s risk score and condition-level stats.

---

## Chatbot engine: state, intents, and response assembly

The **ChatEngine** is stateful per session: it holds a **state** dict (vitals, conditions, and optional form fields like chief complaint and disposition), a **models** dict (the loaded XGBoost classifier), and preloaded **stats** (stats.json). Each incoming message is first **intent-classified** by a rule-based classifier (`med_proj/chatbot/intents.py`): greeting, help, reset (“new patient”), ask (clinical question), update (refine values), or assess (default for patient descriptions and anything that looks like clinical input). For **assess** and **update**, the message is run through **extractors** (`med_proj/chatbot/extractors.py`): regex and synonym-based logic that pulls out age, sex, vitals (temp, pulse, BP, resp, SpO2, pain), triage/ESI, and condition flags (CHF, COPD, CKD, etc.) and merges them into `engine.state`. The API can also merge **structured state** from an external source (e.g. parsed ED form) via `merge_state` on the chat request, so an uploaded PDF’s parsed fields are merged into the same state before `respond()` is called. The engine then **infers missing fields** (e.g. triage from conditions and vitals if not present), builds a **feature row** that aligns to the XGBoost model’s feature names (filling missing features with NaN), runs **prediction** to get a base readmission probability, and applies a **clinical risk adjustment** (evidence-based log-odds shifts for conditions, age, vitals, triage, chronic burden) to produce the final probability. The **reply** is assembled from: (1) a **Patient Summary** (formatted vitals and conditions from state, plus any ED-form fields like chief complaint and disposition), (2) **Readmission Risk** (percentage and a text bar), (3) **Recommendations** (RAG excerpts from the synthetic condition/risk query), and (4) **Condition risk section** (NHAMCS stats for the patient’s conditions). For **ask** intent, the reply is only the RAG results for the user’s question. For **reset**, state is cleared; for **help** and **greeting**, fixed copy is returned.

---

## API and presentation layer

The **FastAPI** service (`med_proj/service/api.py`) exposes: **GET /** → redirect to **GET /static/index.html** (single-page app); **GET /health** → JSON with status, `models_loaded`, and `rag_index_loaded`; **POST /chat** → JSON body `{ message, session_id?, merge_state? }`, returns `{ session_id, reply }` (session-scoped ChatEngine, merge_state merged into engine.state, reply is markdown); **POST /parse-ed-document** → multipart PDF upload, returns `{ parsed, summary }` (parsed is the full structured state from the ED form parser, summary is a short line for the chat bubble); **GET /stats** → precomputed stats JSON for the Stats tab. The **frontend** (static `index.html`) maintains a session id, sends user messages (and optional `merge_state` after PDF upload) to **/chat**, and renders the bot’s markdown reply (headings, lists, bold) in the chat bubble. PDF upload sends the file to **/parse-ed-document**, then calls **/chat** with the summary as the message and the parsed object as `merge_state`, so the next response is a full assessment using the form’s fields. Thus the **technical flow for presentation** is: ingestion produces model + stats + RAG indexes; at runtime the API loads the model and RAG availability, and the chatbot combines state (typed or from PDF), XGBoost prediction, clinical adjustment, and RAG retrieval to produce a single markdown response that the UI displays as the Patient Summary, risk, and recommendations.
