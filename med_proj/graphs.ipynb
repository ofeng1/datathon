{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b685286",
   "metadata": {},
   "source": [
    "Plots for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b75bc4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lifeh\\OneDrive\\Desktop\\data4good-2026\\datathon\\med_proj\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from data.data_loader import DataLoader\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "763863c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lifeh\\OneDrive\\Desktop\\data4good-2026\\datathon\\med_proj\\data\n"
     ]
    }
   ],
   "source": [
    "%cd data/\n",
    "data_loader = DataLoader()\n",
    "zip_paths = [\"ed2015-sas.sas7bdat.zip\", \"ed2016_sas.zip\", \"ed2017_sas.zip\", \"ed2018_sas.zip\",\n",
    "                \"ed2019_sas.zip\", \"ed2020_sas.zip\", \"ed2021_sas.zip\"]\n",
    "df = pd.DataFrame()\n",
    "for zip_path in zip_paths:\n",
    "    df_temp = data_loader.load_data(zip_path)\n",
    "    df = pd.concat([df, df_temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba72ee6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147427, 446)\n",
      "(147427, 1031)\n"
     ]
    }
   ],
   "source": [
    "half_of_rows = len(df) * 0.5\n",
    "df_pruned = df.dropna(thresh = half_of_rows, axis=1)\n",
    "print(df_pruned.shape)\n",
    "print(df.shape)\n",
    "\n",
    "#correlations = df_pruned.corr()['RETRNED'].abs().sort_values(ascending=False)\n",
    "#print(correlations.head(21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b2dc3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RETRNED      1.000000\n",
      "REGION       0.113377\n",
      "TOTHRDIVR    0.106203\n",
      "NOFU         0.098290\n",
      "ADMITHOS     0.086705\n",
      "HDSTAT       0.083240\n",
      "HDDIAG1R     0.082456\n",
      "ADISP        0.080142\n",
      "ADMIT        0.078522\n",
      "HOSPCODE     0.078226\n",
      "LOS          0.076490\n",
      "SUTURE       0.070204\n",
      "RACERFL      0.065644\n",
      "RACEUN       0.063709\n",
      "EOUTINFOE    0.062945\n",
      "IMMEDR       0.062601\n",
      "RESPRD       0.061411\n",
      "EPTRECE      0.061332\n",
      "ADMTPHYS     0.061204\n",
      "RESINT       0.060393\n",
      "IVFLUIDS     0.059568\n",
      "Name: RETRNED, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "correlations = df_pruned.corr(numeric_only=True)['RETRNED'].abs().sort_values(ascending=False)\n",
    "print(correlations.head(21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47f657ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between Ambulance Diversion Hours and Readmittance: 0.2350\n"
     ]
    }
   ],
   "source": [
    "# 1. Filter the dataset to ONLY keep rows where TOTHRDIVR is 2, 3, or 4\n",
    "# This drops all the -9 to -6 values, AND the 5 (data not available)\n",
    "clean_diversion_df = df[df['TOTHRDIVR'].isin([2, 3, 4])]\n",
    "\n",
    "# 2. Calculate the correlation between the cleaned column and readmittance\n",
    "correlation = clean_diversion_df['TOTHRDIVR'].corr(clean_diversion_df['RETRNED'])\n",
    "\n",
    "print(f\"Correlation between Ambulance Diversion Hours and Readmittance: {correlation:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19e8235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147427,)\n",
      "--- Top Correlated Clinical Features with Readmittance ---\n",
      "COMSTAT29    0.670820\n",
      "COMSTAT23    0.430007\n",
      "COMSTAT27    0.284463\n",
      "ADMITHOS     0.086705\n",
      "COMSTAT26    0.072836\n",
      "COMSTAT9     0.069184\n",
      "COMSTAT25    0.063564\n",
      "COMSTAT12    0.063471\n",
      "COMSTAT11    0.060895\n",
      "AGEDAYS      0.060664\n",
      "COMSTAT10    0.056763\n",
      "COMSTAT24    0.055972\n",
      "COMSTAT17    0.055415\n",
      "COMSTAT7     0.053268\n",
      "COMSTAT16    0.051019\n",
      "COMSTAT6     0.049456\n",
      "COMSTAT22    0.048773\n",
      "COMSTAT13    0.045633\n",
      "COMSTAT20    0.043863\n",
      "COMSTAT19    0.043228\n",
      "Name: RETRNED, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Define your specific features\n",
    "comstat_cols = [f'COMSTAT{i}' for i in range(1, 31)]\n",
    "other_cols = [\n",
    "    'CAUSE1', 'CAUSE2', 'CAUSE3', \n",
    "    'AGE', 'AGEDAYS', 'AGER', \n",
    "    'CHF', 'ASTHMA', 'CKD', 'CAD', \n",
    "    'ADMITHOS', 'BOARDHOS', \n",
    "    'BPSYS', 'BPDIAS', 'BPAP',\n",
    "    'RETRNED' # Don't forget the target variable!\n",
    "]\n",
    "\n",
    "all_target_cols = comstat_cols + other_cols\n",
    "\n",
    "# 2. Keep only the columns that actually exist in your dataframe\n",
    "# This prevents KeyError if your specific year's dataset is missing a column\n",
    "existing_cols = [col for col in all_target_cols if col in df.columns]\n",
    "df_subset = df[existing_cols].copy()\n",
    "\n",
    "# 3. Clean the CDC's negative \"missing data\" codes\n",
    "# We replace -9 (Blank), -8 (Unknown), and -7 (Not Applicable) with NaN\n",
    "# Note: Blood pressure (BPSYS) can't naturally be negative, so this is safe.\n",
    "df_subset = df_subset.replace([-9, -8, -7], np.nan)\n",
    "\n",
    "# 4. Calculate correlation with RETRNED\n",
    "# numeric_only=True skips the alphanumeric CAUSE codes so it doesn't crash\n",
    "correlations = df_subset.corr(numeric_only=True)['RETRNED'].abs().sort_values(ascending=False)\n",
    "\n",
    "# 5. Drop RETRNED from the results (since it correlates 1.0 with itself) \n",
    "# and print the top 20 most correlated features\n",
    "print(\"--- Top Correlated Clinical Features with Readmittance ---\")\n",
    "print(correlations.drop('RETRNED', errors='ignore').head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2be816b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in original dataset: 147427\n",
      "Valid values left in COMSTAT29: 77\n",
      "2544     2.0\n",
      "2655     1.0\n",
      "7953     2.0\n",
      "15004    1.0\n",
      "15010    1.0\n",
      "17286    1.0\n",
      "17332    1.0\n",
      "17898    1.0\n",
      "17928    1.0\n",
      "19652    1.0\n",
      "20886    1.0\n",
      "2544     2.0\n",
      "2655     1.0\n",
      "7953     2.0\n",
      "15004    1.0\n",
      "15010    1.0\n",
      "17286    1.0\n",
      "17332    1.0\n",
      "17898    1.0\n",
      "17928    1.0\n",
      "19652    1.0\n",
      "20886    1.0\n",
      "2544     2.0\n",
      "2655     1.0\n",
      "7953     2.0\n",
      "15004    1.0\n",
      "15010    1.0\n",
      "17286    1.0\n",
      "17332    1.0\n",
      "17898    1.0\n",
      "17928    1.0\n",
      "19652    1.0\n",
      "20886    1.0\n",
      "2544     2.0\n",
      "2655     1.0\n",
      "7953     2.0\n",
      "15004    1.0\n",
      "15010    1.0\n",
      "17286    1.0\n",
      "17332    1.0\n",
      "17898    1.0\n",
      "17928    1.0\n",
      "19652    1.0\n",
      "20886    1.0\n",
      "2544     2.0\n",
      "2655     1.0\n",
      "7953     2.0\n",
      "15004    1.0\n",
      "15010    1.0\n",
      "17286    1.0\n",
      "17332    1.0\n",
      "17898    1.0\n",
      "17928    1.0\n",
      "19652    1.0\n",
      "20886    1.0\n",
      "2544     2.0\n",
      "2655     1.0\n",
      "7953     2.0\n",
      "15004    1.0\n",
      "15010    1.0\n",
      "17286    1.0\n",
      "17332    1.0\n",
      "17898    1.0\n",
      "17928    1.0\n",
      "19652    1.0\n",
      "20886    1.0\n",
      "2544     2.0\n",
      "2655     1.0\n",
      "7953     2.0\n",
      "15004    1.0\n",
      "15010    1.0\n",
      "17286    1.0\n",
      "17332    1.0\n",
      "17898    1.0\n",
      "17928    1.0\n",
      "19652    1.0\n",
      "20886    1.0\n",
      "Name: COMSTAT29, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Force the column to be numerical. \n",
    "# errors='coerce' turns any text or weird byte-strings (like b'710-') directly into NaN\n",
    "clean_col = pd.to_numeric(df['COMSTAT29'], errors='coerce')\n",
    "\n",
    "# 2. Filter out the CDC's negative missing codes (keep only values > 0)\n",
    "# COMSTAT valid values are usually 1, 2, or 3\n",
    "clean_col = clean_col[clean_col > 0]\n",
    "\n",
    "# 3. Count how many valid rows are left\n",
    "remaining_count = clean_col.count()\n",
    "\n",
    "print(f\"Total rows in original dataset: {len(df)}\")\n",
    "print(f\"Valid values left in COMSTAT29: {remaining_count}\")\n",
    "\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    print(clean_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4920fcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in original dataset: 147427\n",
      "Valid values left in COMSTAT23: 378\n",
      "185      1.0\n",
      "781      1.0\n",
      "795      1.0\n",
      "816      1.0\n",
      "823      1.0\n",
      "2544     1.0\n",
      "2564     1.0\n",
      "2594     1.0\n",
      "2655     1.0\n",
      "4049     1.0\n",
      "5904     1.0\n",
      "6595     1.0\n",
      "6614     1.0\n",
      "6633     1.0\n",
      "6634     1.0\n",
      "6646     1.0\n",
      "6676     1.0\n",
      "6701     1.0\n",
      "7953     2.0\n",
      "8054     1.0\n",
      "11422    1.0\n",
      "13761    1.0\n",
      "13774    1.0\n",
      "14921    1.0\n",
      "14936    2.0\n",
      "14951    2.0\n",
      "14978    1.0\n",
      "14985    2.0\n",
      "15004    1.0\n",
      "15008    1.0\n",
      "15010    1.0\n",
      "15023    1.0\n",
      "15024    1.0\n",
      "17283    1.0\n",
      "17286    1.0\n",
      "17332    1.0\n",
      "17891    1.0\n",
      "17896    1.0\n",
      "17898    1.0\n",
      "17928    1.0\n",
      "17930    1.0\n",
      "17940    1.0\n",
      "19626    1.0\n",
      "19627    1.0\n",
      "19652    2.0\n",
      "19659    1.0\n",
      "19668    1.0\n",
      "19682    1.0\n",
      "20707    1.0\n",
      "20747    1.0\n",
      "20822    1.0\n",
      "20859    1.0\n",
      "20886    1.0\n",
      "20979    1.0\n",
      "185      1.0\n",
      "781      1.0\n",
      "795      1.0\n",
      "816      1.0\n",
      "823      1.0\n",
      "2544     1.0\n",
      "2564     1.0\n",
      "2594     1.0\n",
      "2655     1.0\n",
      "4049     1.0\n",
      "5904     1.0\n",
      "6595     1.0\n",
      "6614     1.0\n",
      "6633     1.0\n",
      "6634     1.0\n",
      "6646     1.0\n",
      "6676     1.0\n",
      "6701     1.0\n",
      "7953     2.0\n",
      "8054     1.0\n",
      "11422    1.0\n",
      "13761    1.0\n",
      "13774    1.0\n",
      "14921    1.0\n",
      "14936    2.0\n",
      "14951    2.0\n",
      "14978    1.0\n",
      "14985    2.0\n",
      "15004    1.0\n",
      "15008    1.0\n",
      "15010    1.0\n",
      "15023    1.0\n",
      "15024    1.0\n",
      "17283    1.0\n",
      "17286    1.0\n",
      "17332    1.0\n",
      "17891    1.0\n",
      "17896    1.0\n",
      "17898    1.0\n",
      "17928    1.0\n",
      "17930    1.0\n",
      "17940    1.0\n",
      "19626    1.0\n",
      "19627    1.0\n",
      "19652    2.0\n",
      "19659    1.0\n",
      "19668    1.0\n",
      "19682    1.0\n",
      "20707    1.0\n",
      "20747    1.0\n",
      "20822    1.0\n",
      "20859    1.0\n",
      "20886    1.0\n",
      "20979    1.0\n",
      "185      1.0\n",
      "781      1.0\n",
      "795      1.0\n",
      "816      1.0\n",
      "823      1.0\n",
      "2544     1.0\n",
      "2564     1.0\n",
      "2594     1.0\n",
      "2655     1.0\n",
      "4049     1.0\n",
      "5904     1.0\n",
      "6595     1.0\n",
      "6614     1.0\n",
      "6633     1.0\n",
      "6634     1.0\n",
      "6646     1.0\n",
      "6676     1.0\n",
      "6701     1.0\n",
      "7953     2.0\n",
      "8054     1.0\n",
      "11422    1.0\n",
      "13761    1.0\n",
      "13774    1.0\n",
      "14921    1.0\n",
      "14936    2.0\n",
      "14951    2.0\n",
      "14978    1.0\n",
      "14985    2.0\n",
      "15004    1.0\n",
      "15008    1.0\n",
      "15010    1.0\n",
      "15023    1.0\n",
      "15024    1.0\n",
      "17283    1.0\n",
      "17286    1.0\n",
      "17332    1.0\n",
      "17891    1.0\n",
      "17896    1.0\n",
      "17898    1.0\n",
      "17928    1.0\n",
      "17930    1.0\n",
      "17940    1.0\n",
      "19626    1.0\n",
      "19627    1.0\n",
      "19652    2.0\n",
      "19659    1.0\n",
      "19668    1.0\n",
      "19682    1.0\n",
      "20707    1.0\n",
      "20747    1.0\n",
      "20822    1.0\n",
      "20859    1.0\n",
      "20886    1.0\n",
      "20979    1.0\n",
      "185      1.0\n",
      "781      1.0\n",
      "795      1.0\n",
      "816      1.0\n",
      "823      1.0\n",
      "2544     1.0\n",
      "2564     1.0\n",
      "2594     1.0\n",
      "2655     1.0\n",
      "4049     1.0\n",
      "5904     1.0\n",
      "6595     1.0\n",
      "6614     1.0\n",
      "6633     1.0\n",
      "6634     1.0\n",
      "6646     1.0\n",
      "6676     1.0\n",
      "6701     1.0\n",
      "7953     2.0\n",
      "8054     1.0\n",
      "11422    1.0\n",
      "13761    1.0\n",
      "13774    1.0\n",
      "14921    1.0\n",
      "14936    2.0\n",
      "14951    2.0\n",
      "14978    1.0\n",
      "14985    2.0\n",
      "15004    1.0\n",
      "15008    1.0\n",
      "15010    1.0\n",
      "15023    1.0\n",
      "15024    1.0\n",
      "17283    1.0\n",
      "17286    1.0\n",
      "17332    1.0\n",
      "17891    1.0\n",
      "17896    1.0\n",
      "17898    1.0\n",
      "17928    1.0\n",
      "17930    1.0\n",
      "17940    1.0\n",
      "19626    1.0\n",
      "19627    1.0\n",
      "19652    2.0\n",
      "19659    1.0\n",
      "19668    1.0\n",
      "19682    1.0\n",
      "20707    1.0\n",
      "20747    1.0\n",
      "20822    1.0\n",
      "20859    1.0\n",
      "20886    1.0\n",
      "20979    1.0\n",
      "185      1.0\n",
      "781      1.0\n",
      "795      1.0\n",
      "816      1.0\n",
      "823      1.0\n",
      "2544     1.0\n",
      "2564     1.0\n",
      "2594     1.0\n",
      "2655     1.0\n",
      "4049     1.0\n",
      "5904     1.0\n",
      "6595     1.0\n",
      "6614     1.0\n",
      "6633     1.0\n",
      "6634     1.0\n",
      "6646     1.0\n",
      "6676     1.0\n",
      "6701     1.0\n",
      "7953     2.0\n",
      "8054     1.0\n",
      "11422    1.0\n",
      "13761    1.0\n",
      "13774    1.0\n",
      "14921    1.0\n",
      "14936    2.0\n",
      "14951    2.0\n",
      "14978    1.0\n",
      "14985    2.0\n",
      "15004    1.0\n",
      "15008    1.0\n",
      "15010    1.0\n",
      "15023    1.0\n",
      "15024    1.0\n",
      "17283    1.0\n",
      "17286    1.0\n",
      "17332    1.0\n",
      "17891    1.0\n",
      "17896    1.0\n",
      "17898    1.0\n",
      "17928    1.0\n",
      "17930    1.0\n",
      "17940    1.0\n",
      "19626    1.0\n",
      "19627    1.0\n",
      "19652    2.0\n",
      "19659    1.0\n",
      "19668    1.0\n",
      "19682    1.0\n",
      "20707    1.0\n",
      "20747    1.0\n",
      "20822    1.0\n",
      "20859    1.0\n",
      "20886    1.0\n",
      "20979    1.0\n",
      "185      1.0\n",
      "781      1.0\n",
      "795      1.0\n",
      "816      1.0\n",
      "823      1.0\n",
      "2544     1.0\n",
      "2564     1.0\n",
      "2594     1.0\n",
      "2655     1.0\n",
      "4049     1.0\n",
      "5904     1.0\n",
      "6595     1.0\n",
      "6614     1.0\n",
      "6633     1.0\n",
      "6634     1.0\n",
      "6646     1.0\n",
      "6676     1.0\n",
      "6701     1.0\n",
      "7953     2.0\n",
      "8054     1.0\n",
      "11422    1.0\n",
      "13761    1.0\n",
      "13774    1.0\n",
      "14921    1.0\n",
      "14936    2.0\n",
      "14951    2.0\n",
      "14978    1.0\n",
      "14985    2.0\n",
      "15004    1.0\n",
      "15008    1.0\n",
      "15010    1.0\n",
      "15023    1.0\n",
      "15024    1.0\n",
      "17283    1.0\n",
      "17286    1.0\n",
      "17332    1.0\n",
      "17891    1.0\n",
      "17896    1.0\n",
      "17898    1.0\n",
      "17928    1.0\n",
      "17930    1.0\n",
      "17940    1.0\n",
      "19626    1.0\n",
      "19627    1.0\n",
      "19652    2.0\n",
      "19659    1.0\n",
      "19668    1.0\n",
      "19682    1.0\n",
      "20707    1.0\n",
      "20747    1.0\n",
      "20822    1.0\n",
      "20859    1.0\n",
      "20886    1.0\n",
      "20979    1.0\n",
      "185      1.0\n",
      "781      1.0\n",
      "795      1.0\n",
      "816      1.0\n",
      "823      1.0\n",
      "2544     1.0\n",
      "2564     1.0\n",
      "2594     1.0\n",
      "2655     1.0\n",
      "4049     1.0\n",
      "5904     1.0\n",
      "6595     1.0\n",
      "6614     1.0\n",
      "6633     1.0\n",
      "6634     1.0\n",
      "6646     1.0\n",
      "6676     1.0\n",
      "6701     1.0\n",
      "7953     2.0\n",
      "8054     1.0\n",
      "11422    1.0\n",
      "13761    1.0\n",
      "13774    1.0\n",
      "14921    1.0\n",
      "14936    2.0\n",
      "14951    2.0\n",
      "14978    1.0\n",
      "14985    2.0\n",
      "15004    1.0\n",
      "15008    1.0\n",
      "15010    1.0\n",
      "15023    1.0\n",
      "15024    1.0\n",
      "17283    1.0\n",
      "17286    1.0\n",
      "17332    1.0\n",
      "17891    1.0\n",
      "17896    1.0\n",
      "17898    1.0\n",
      "17928    1.0\n",
      "17930    1.0\n",
      "17940    1.0\n",
      "19626    1.0\n",
      "19627    1.0\n",
      "19652    2.0\n",
      "19659    1.0\n",
      "19668    1.0\n",
      "19682    1.0\n",
      "20707    1.0\n",
      "20747    1.0\n",
      "20822    1.0\n",
      "20859    1.0\n",
      "20886    1.0\n",
      "20979    1.0\n",
      "Name: COMSTAT23, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 1. Force the column to be numerical. \n",
    "# errors='coerce' turns any text or weird byte-strings (like b'710-') directly into NaN\n",
    "clean_col = pd.to_numeric(df['COMSTAT23'], errors='coerce')\n",
    "\n",
    "# 2. Filter out the CDC's negative missing codes (keep only values > 0)\n",
    "# COMSTAT valid values are usually 1, 2, or 3\n",
    "clean_col = clean_col[clean_col > 0]\n",
    "\n",
    "# 3. Count how many valid rows are left\n",
    "remaining_count = clean_col.count()\n",
    "\n",
    "print(f\"Total rows in original dataset: {len(df)}\")\n",
    "print(f\"Valid values left in COMSTAT23: {remaining_count}\")\n",
    "\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    print(clean_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "596a4563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between AGE and Readmittance: -0.0228\n"
     ]
    }
   ],
   "source": [
    "df_age_check = df[['AGE', 'RETRNED']].copy()\n",
    "\n",
    "# 2. Replace any CDC negative \"missing data\" codes with NaN so they are ignored in the math\n",
    "df_age_check = df_age_check.replace([-9, -8, -7], np.nan)\n",
    "\n",
    "# 3. Calculate the correlation\n",
    "age_corr = df_age_check['AGE'].corr(df_age_check['RETRNED'])\n",
    "\n",
    "print(f\"Correlation between AGE and Readmittance: {age_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e02fb6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RFE... this might take a few seconds.\n",
      "\n",
      "--- RFE Results ---\n",
      "      Feature  Kept by RFE  Ranking\n",
      "3    ADMITHOS         True        1\n",
      "5    COMSTAT9         True        1\n",
      "13   COMSTAT7         True        1\n",
      "9     AGEDAYS         True        1\n",
      "15   COMSTAT6         True        1\n",
      "8   COMSTAT11        False        2\n",
      "10  COMSTAT10        False        3\n",
      "0   COMSTAT29        False        4\n",
      "17  COMSTAT13        False        5\n",
      "7   COMSTAT12        False        6\n",
      "14  COMSTAT16        False        7\n",
      "2   COMSTAT27        False        8\n",
      "19  COMSTAT19        False        9\n",
      "18  COMSTAT20        False       10\n",
      "12  COMSTAT17        False       11\n",
      "1   COMSTAT23        False       12\n",
      "4   COMSTAT26        False       13\n",
      "16  COMSTAT22        False       14\n",
      "11  COMSTAT24        False       15\n",
      "6   COMSTAT25        False       16\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Define the top features you found\n",
    "top_features = [\n",
    "    'COMSTAT29', 'COMSTAT23', 'COMSTAT27', 'ADMITHOS', 'COMSTAT26', \n",
    "    'COMSTAT9', 'COMSTAT25', 'COMSTAT12', 'COMSTAT11', 'AGEDAYS', \n",
    "    'COMSTAT10', 'COMSTAT24', 'COMSTAT17', 'COMSTAT7', 'COMSTAT16', \n",
    "    'COMSTAT6', 'COMSTAT22', 'COMSTAT13', 'COMSTAT20', 'COMSTAT19'\n",
    "]\n",
    "\n",
    "# 2. Prep your Data (RFE algorithms will crash if there are NaNs)\n",
    "# We fill NaNs with -1 so the Random Forest knows it represents \"missing/blank\"\n",
    "X = df[top_features].fillna(-1)\n",
    "\n",
    "# Ensure your target variable has no missing values either\n",
    "# We'll drop rows where the target (RETRNED) is unknown\n",
    "valid_y_index = df['RETRNED'].dropna().index\n",
    "X = X.loc[valid_y_index]\n",
    "y = df['RETRNED'].loc[valid_y_index]\n",
    "\n",
    "# 3. Set up the Base Model\n",
    "# Random Forest is great here because it handles the -1 \"missing\" values well\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# 4. Set up and Train RFE\n",
    "# Let's tell RFE to keep only the top 5 absolute best features\n",
    "print(\"Training RFE... this might take a few seconds.\")\n",
    "rfe_selector = RFE(estimator=model, n_features_to_select=5, step=1)\n",
    "rfe_selector = rfe_selector.fit(X, y)\n",
    "\n",
    "# 5. Display the Results\n",
    "results = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Kept by RFE': rfe_selector.support_, # True if it's in the top 5\n",
    "    'Ranking': rfe_selector.ranking_      # 1 means top feature, 2 is next to be dropped, etc.\n",
    "}).sort_values(by='Ranking')\n",
    "\n",
    "print(\"\\n--- RFE Results ---\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3baed7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model Accuracy: 92.50% ---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      1.00      0.96    190787\n",
      "         1.0       0.70      0.01      0.03     15611\n",
      "\n",
      "    accuracy                           0.92    206398\n",
      "   macro avg       0.81      0.51      0.49    206398\n",
      "weighted avg       0.91      0.92      0.89    206398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. Define your winning features from RFE\n",
    "best_features = ['ADMITHOS', 'COMSTAT9', 'COMSTAT7', 'AGEDAYS', 'COMSTAT6']\n",
    "\n",
    "# 2. Prep the data (fill missing values with -1 so the model doesn't crash)\n",
    "X = df[best_features].fillna(-1)\n",
    "\n",
    "# Ensure target variable has no missing values\n",
    "valid_indices = df['RETRNED'].dropna().index\n",
    "X = X.loc[valid_indices]\n",
    "y = df['RETRNED'].loc[valid_indices]\n",
    "\n",
    "# 3. Split the data (80% for training, 20% for testing)\n",
    "# stratify=y ensures the 80/20 split keeps the same ratio of readmitted patients\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 4. Train the Random Forest Model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Make predictions on the unseen test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 6. Grade the results\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"--- Model Accuracy: {accuracy * 100:.2f}% ---\\n\")\n",
    "\n",
    "# Print the detailed breakdown\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a159fe55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 92.50%\n",
      "Testing Accuracy:  92.50%\n",
      "Difference (Overfit Gap): 0.01%\n"
     ]
    }
   ],
   "source": [
    "# 1. Get the accuracy on the data the model was TRAINED on\n",
    "y_train_pred = model.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "# 2. You already have the test accuracy from the previous step\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Testing Accuracy:  {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# 3. Calculate the gap\n",
    "print(f\"Difference (Overfit Gap): {(train_accuracy - test_accuracy) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4fd09892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Log Loss: 0.2557\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# 1. Ask the model for the PROBABILITY of readmission, not just a 0 or 1\n",
    "# predict_proba returns two columns: [Probability of 0, Probability of 1]\n",
    "y_pred_probs = model.predict_proba(X_test)\n",
    "\n",
    "# 2. Calculate the Log Loss\n",
    "# A lower log loss means a better, more confident model!\n",
    "rf_loss = log_loss(y_test, y_pred_probs)\n",
    "\n",
    "print(f\"Random Forest Log Loss: {rf_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d5385b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New Patient Prediction ---\n",
      "âœ… Low Risk: The model predicts this patient will NOT be readmitted.\n",
      "Exact Readmission Risk Score: 4.3%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Create a new \"unseen\" patient record.\n",
    "# We put the data in a DataFrame so the column names match perfectly.\n",
    "# Remember: We filled missing data with -1 during training, so do the same here if a value is unknown!\n",
    "new_patient = pd.DataFrame([{\n",
    "    'ADMITHOS': 0,    # Example: 1 if they were admitted to the hospital, 0 if not\n",
    "    'COMSTAT9': 1,   # Example: -1 because they didn't get a 9th medication\n",
    "    'COMSTAT7': 1,   # Example: -1 because they didn't get a 7th medication\n",
    "    'AGEDAYS': 364,    # Example: -1 (CDC uses this for infants, usually blank for adults)\n",
    "    'COMSTAT6': 2     # Example: 1 meaning their 6th drug was a single-entity drug\n",
    "}])\n",
    "\n",
    "# 2. Get the \"Hard\" Prediction (0 or 1)\n",
    "# 0 = Will NOT return, 1 = WILL return\n",
    "prediction = model.predict(new_patient)\n",
    "\n",
    "# 3. Get the \"Soft\" Prediction (Probability / Risk Score)\n",
    "# In healthcare, doctors don't just want a 0 or 1; they want a risk percentage!\n",
    "# predict_proba returns a list: [Probability of 0, Probability of 1]\n",
    "probabilities = model.predict_proba(new_patient)\n",
    "risk_score = probabilities[0][1] * 100 # Grabbing the probability of '1' and converting to %\n",
    "\n",
    "# 4. Print the results clearly\n",
    "print(\"--- New Patient Prediction ---\")\n",
    "if prediction[0] == 1:\n",
    "    print(\"ðŸš¨ High Risk: The model predicts this patient WILL be readmitted.\")\n",
    "else:\n",
    "    print(\"âœ… Low Risk: The model predicts this patient will NOT be readmitted.\")\n",
    "\n",
    "print(f\"Exact Readmission Risk Score: {risk_score:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c46e810a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model Performance (Clinical Features) ---\n",
      "Overall Accuracy: 80.57%\n",
      "\n",
      "--- Detailed Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.83      0.89     27256\n",
      "         1.0       0.20      0.54      0.30      2230\n",
      "\n",
      "    accuracy                           0.81     29486\n",
      "   macro avg       0.58      0.68      0.59     29486\n",
      "weighted avg       0.90      0.81      0.84     29486\n",
      "\n",
      "--- Confusion Matrix ---\n",
      "[[22552  4704]\n",
      " [ 1024  1206]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# 1. Define our new, robust clinical features\n",
    "clinical_features = [\n",
    "    'AGE', 'IMMEDR', 'INJURY', \n",
    "    'BPSYS', 'PULSE', \n",
    "    'CHF', 'COPD', 'DIABETES', 'CKD', 'ASTHMA'\n",
    "]\n",
    "target = 'RETRNED'\n",
    "\n",
    "# 2. Keep only columns that exist in your specific dataset to prevent errors\n",
    "existing_features = [col for col in clinical_features if col in df.columns]\n",
    "df_model = df[existing_features + [target]].copy()\n",
    "\n",
    "# 3. Clean the CDC negative missing codes\n",
    "df_model = df_model.replace([-9, -8, -7], np.nan)\n",
    "df_model = df_model.dropna(subset=[target])\n",
    "\n",
    "# Fill missing feature data with -1 so the tree can process them as \"Unknown\"\n",
    "X = df_model[existing_features].fillna(-1)\n",
    "y = df_model[target]\n",
    "\n",
    "# 4. Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 5. Train the Model (Keeping class_weight='balanced')\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    max_depth=10, \n",
    "    random_state=42, \n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# 6. Make Predictions & Grade the Model\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "print(\"--- Model Performance (Clinical Features) ---\")\n",
    "print(f\"Overall Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\\n\")\n",
    "print(\"--- Detailed Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"--- Confusion Matrix ---\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "be40e2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after filtering: 147427\n",
      "High-Risk Senior Corrected Risk: 1.0%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# --- STEP 1: DYNAMIC COLUMN CHECK ---\n",
    "# NHAMCS columns can vary by year. Let's find the right ones.\n",
    "cols = df.columns.tolist()\n",
    "\n",
    "admit_col = 'ADMITHOS' if 'ADMITHOS' in cols else ('ADMIT' if 'ADMIT' in cols else None)\n",
    "return_col = 'RETRNED' if 'RETRNED' in cols else ('RETVISIT' if 'RETVISIT' in cols else None)\n",
    "\n",
    "# --- STEP 2: SAFE FILTERING ---\n",
    "if admit_col:\n",
    "    # Try to filter for discharged patients, but if it wipes out the data, keep all\n",
    "    temp_filter = df[df[admit_col].isin([2, 3])].copy()\n",
    "    df_filtered = temp_filter if len(temp_filter) > 0 else df.copy()\n",
    "else:\n",
    "    df_filtered = df.copy()\n",
    "\n",
    "# Ensure we have data\n",
    "print(f\"Rows after filtering: {len(df_filtered)}\")\n",
    "\n",
    "# --- STEP 3: FEATURE MAPPING ---\n",
    "# Use the comorbidity and age variables you identified\n",
    "chronic_flags = [c for c in ['CHF', 'ASTHMA', 'CKD', 'CAD'] if c in cols]\n",
    "comstat_features = [f'COMSTAT{i}' for i in range(1, 11) if f'COMSTAT{i}' in cols]\n",
    "vitals = [v for v in ['BPSYS', 'PULSE'] if v in cols]\n",
    "\n",
    "features = ['AGE'] + chronic_flags + comstat_features + vitals\n",
    "\n",
    "# Clean target\n",
    "if return_col:\n",
    "    df_filtered['target'] = (df_filtered[return_col] == 1).astype(int)\n",
    "else:\n",
    "    raise ValueError(\"Could not find the Return/Readmission column in your dataset!\")\n",
    "\n",
    "# Final check to prevent n_samples=0\n",
    "if len(df_filtered) < 10:\n",
    "    raise ValueError(\"Dataset is too small after filtering. Check your ADMITHOS codes.\")\n",
    "\n",
    "# --- STEP 4: TRAIN WITH WEIGHTS ---\n",
    "X = df_filtered[features].fillna(0) # Fill NaNs to prevent crashes\n",
    "y = df_filtered['target']\n",
    "weights = np.where((df_filtered['AGE'] > 65) & (df_filtered['target'] == 1), 15.0, 1.0)\n",
    "\n",
    "# Unpack carefully\n",
    "X_train, X_test, y_train, y_test, w_train, w_test = train_test_split(\n",
    "    X, y, weights, test_size=0.2, stratify=y if y.nunique() > 1 else None, random_state=42\n",
    ")\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "rf_model.fit(X_train, y_train, sample_weight=w_train)\n",
    "\n",
    "# --- STEP 5: PREDICT ---\n",
    "# Create a dummy row for the Senior to ensure columns match exactly\n",
    "senior_profile = pd.DataFrame(0, index=[0], columns=features)\n",
    "senior_profile['AGE'] = 82\n",
    "if 'CHF' in features: senior_profile['CHF'] = 1\n",
    "if 'BPSYS' in features: senior_profile['BPSYS'] = 175\n",
    "\n",
    "risk = rf_model.predict_proba(senior_profile)[0][1]\n",
    "print(f\"High-Risk Senior Corrected Risk: {risk*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26b5c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If this risk is above 20%, we would flag this patient for extra care coordination!\n",
      "This is a simplified example, but in a real hospital setting, we'd use this risk score to trigger interventions like follow-up calls, home health visits, or medication reviews to help prevent readmission.\n"
     ]
    }
   ],
   "source": [
    "print(\"If this risk is above 20%, we would flag this patient for extra care coordination!\")\n",
    "print(\"This is a simplified example, but in a real hospital setting, we'd use this risk score to trigger interventions like follow-up calls, home health visits, or medication reviews to help prevent readmission.\")\n",
    "print(\"nothing\")\n",
    "print(\"something\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dddd38f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lifeh\\AppData\\Local\\Temp\\ipykernel_28596\\3695005923.py:38: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X = X.replace([-9, -8, -7], np.nan)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the realistic model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [0], got [1.]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     53\u001b[39m model = xgb.XGBClassifier(\n\u001b[32m     54\u001b[39m     n_estimators=\u001b[32m200\u001b[39m,\n\u001b[32m     55\u001b[39m     max_depth=\u001b[32m5\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     61\u001b[39m     n_jobs=-\u001b[32m1\u001b[39m\n\u001b[32m     62\u001b[39m )\n\u001b[32m     64\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining the realistic model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# 7. Evaluate\u001b[39;00m\n\u001b[32m     68\u001b[39m y_proba = model.predict_proba(X_test)[:, \u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lifeh\\OneDrive\\Desktop\\data4good-2026\\.venv\\Lib\\site-packages\\xgboost\\core.py:751\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    750\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m751\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lifeh\\OneDrive\\Desktop\\data4good-2026\\.venv\\Lib\\site-packages\\xgboost\\sklearn.py:1761\u001b[39m, in \u001b[36mXGBClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1756\u001b[39m     expected_classes = \u001b[38;5;28mself\u001b[39m.classes_\n\u001b[32m   1757\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1758\u001b[39m     classes.shape != expected_classes.shape\n\u001b[32m   1759\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (classes == expected_classes).all()\n\u001b[32m   1760\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1761\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1762\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1763\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1764\u001b[39m     )\n\u001b[32m   1766\u001b[39m params = \u001b[38;5;28mself\u001b[39m.get_xgb_params()\n\u001b[32m   1768\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.objective):\n",
      "\u001b[31mValueError\u001b[39m: Invalid classes inferred from unique values of `y`.  Expected: [0], got [1.]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Target Preparation\n",
    "# Standardize RETRNED: 1 = Yes, 0 = No/Other\n",
    "# In NHAMCS, 1 is Yes, 2 is No. Ensure it's 0 and 1 for the model.\n",
    "df_model = df.copy()\n",
    "df_model['target'] = df_model['RETRNED'].map({1: 1, 2: 0})\n",
    "df_model = df_model.dropna(subset=['target'])\n",
    "\n",
    "# 2. Comprehensive Leakage & Metadata Removal\n",
    "# We must remove everything that is determined AFTER the patient is triaged.\n",
    "# This ensures the model only uses \"Entry\" data (History, Vitals, Triage).\n",
    "leaky_prefixes = [\n",
    "    'EOUT', 'ADISP', 'ADMIT', 'HDDIAG', 'RX', 'MED', 'PROC', \n",
    "    'TRAN', 'OBS', 'STAY', 'LOS', 'HDSTAT', 'DOA', 'DIEDED',\n",
    "    'LWBS', 'LBTC', 'LEFTAMA', 'NODISP', 'NOFU', 'RETREFFU'\n",
    "]\n",
    "\n",
    "# Specifically drop ID, year, and weighting columns (Metadata)\n",
    "metadata_cols = ['PATWT', 'EDWT', 'HOSPCODE', 'PATCODE', 'YEAR', 'CSTRATM', 'CPSUM']\n",
    "\n",
    "cols_to_drop = ['RETRNED', 'target']\n",
    "for col in df_model.columns:\n",
    "    if any(col.startswith(pref) for pref in leaky_prefixes):\n",
    "        cols_to_drop.append(col)\n",
    "    if col in metadata_cols:\n",
    "        cols_to_drop.append(col)\n",
    "\n",
    "X = df_model.drop(columns=[c for c in cols_to_drop if c in df_model.columns])\n",
    "y = df_model['target']\n",
    "\n",
    "# 3. Handle NHAMCS Special Missing Codes (-9, -8, -7)\n",
    "X = X.replace([-9, -8, -7], np.nan)\n",
    "\n",
    "# 4. Handle Categorical Features\n",
    "for col in X.select_dtypes(include=['object', 'string']).columns:\n",
    "    X[col] = X[col].astype('category')\n",
    "\n",
    "# 5. Train-Test Split (with stratification to keep return-visit ratio even)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 6. Model Training (Adjusted for Class Imbalance)\n",
    "# Calculate the ratio of Non-Returns to Returns to help the model \"pay attention\" to the 1s\n",
    "ratio = (y == 0).sum() / (y == 1).sum()\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    scale_pos_weight=ratio, # Crucial for improving Recall (finding the return patients)\n",
    "    enable_categorical=True,\n",
    "    eval_metric='auc',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training the realistic model...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 7. Evaluate\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_proba > 0.5).astype(int)\n",
    "\n",
    "print(f\"\\nRealistic ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 8. Feature Importance\n",
    "top_x = 20\n",
    "importances = pd.DataFrame({'Feature': X.columns, 'Importance': model.feature_importances_})\n",
    "importances = importances.sort_values(by='Importance', ascending=False).head(top_x)\n",
    "\n",
    "print(f\"\\n--- Top {top_x} Realistic Predictors ---\")\n",
    "print(importances)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
